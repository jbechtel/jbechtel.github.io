{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "\n",
    "<!-- PELICAN_BEGIN_SUMMARY -->\n",
    "\n",
    "Niave Bayes is a classification technique that assumes strong independence between features. Even under these assumptions, it has been shown to perform well for certain tasks like spam filtering of text based messages. \n",
    "\n",
    "In this post, I will explain the theory behind Naive Bayes. \n",
    "\n",
    "We'll also take a look under the hood of Scikit-learn's implementation of Multinomail Bayes, and then we'll implement a stripped down version that retains the key features in order to understand what's going on.\n",
    "\n",
    "<!-- PELICAN_END_SUMMARY -->\n",
    "\n",
    "\n",
    "## Basic Theory\n",
    "\n",
    "First, we need to define what question we are trying to answer. Let's stick to the SPAM/HAM filtering application. \n",
    "\n",
    "So given a message, what is the probability that is is HAM (normal message) or SPAM? \n",
    "\n",
    "The quantity we are after then is $p(S|D)$. What is the probability of SPAM (S) given a document (D). \n",
    "\n",
    "In the framework of Bayesian probability, this quantity is known as the posterior. Conveniently, Bayes Theorem defines a practical way to calculate this quantity:\n",
    "\n",
    "$$ p(S|D) = \\frac{P(D|S)*P(S)}{P(D)} = \\frac{P(D,S)}{P(D)} $$\n",
    "\n",
    "In words, this expression reads: the probability that a given document, D, belongs to SPAM is equal to the likelihood of observing the document (given that it belongs to SPAM) multiplied by the prior (the probability of drawing a SPAM document for a collection of messages) divided by the probability of observing the document (evidence)\n",
    "\n",
    "In machine learning applications, the most convenient way to represent information about a particular sample is in a vector. In this case of message classification, we can vectorize the message based on word counts from a large vocabulary. In this way we transform our raw .txt data to a vector where each element denotes the number of times a word appears in the message. \n",
    "\n",
    "$$ f: D \\rightarrow \\vec{x} $$\n",
    "\n",
    "Here $f$ is a function that vectorizes the input document D. The length of the resulting feature vector $\\vec{x}$ corresponds to the number of unique words within a large number of sample documents. Again, the values of the vector entries $x_i$ represent the number of times the $i^{\\text{th}}$ word appears in document D. \n",
    "\n",
    "There are more sophisticated ways to encode text into feature vectors, but for now we will stick the feature counts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have transformed our documents into feature vectors we rewrite the joint likelihood as \n",
    "\n",
    "\\begin{align}\n",
    "p(D,S) =& p(\\vec{x},S)\\\\\n",
    "       =& p(x_1 | x_2, x_3, ..., x_n, S)p(x_2, x_3, ..., x_n, S)\\\\\n",
    "       &\\vdots \\\\\n",
    "       =& p(x_1| x_2, x_3, ..., x_n, S)p(x_2 | x_3, ..., x_n, S)... p(x_n | S)\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Here, we make the assumption that the likelihood of a feature is independent of all other likelihoods:\n",
    "\n",
    "$$ p(x_i | x_{i+1}, ..., x_n, S) = p(x_i | S) $$\n",
    "\n",
    "so that \n",
    "\n",
    "$$ p(D,S) = p(\\vec{x},S) = p(S) \\prod_{i=1}^{n}p(x_i | S) $$\n",
    "\n",
    "giving finally\n",
    "\n",
    "\\begin{align}\n",
    "p(S | \\vec{x})&=\\frac{p(S) \\prod_{i=1}^{n}p(x_i | S)}{p(\\vec{x})}\\\\\n",
    "&=\\frac{p(S) \\prod_{i=1}^{n}p(x_i | S)}{\\sum_{k}p(S_k)p(\\vec{x}|S_k)}\\\\\n",
    "&=\\frac{p(S) \\prod_{i=1}^{n}p(x_i | S)}{\\sum_{k}p(S_k)\\prod_{i=1}^{n}p(x_i | S_k)}\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement Naive Bayes, we now need to specify the form $p(\\vec{x} | S)$ where the feature probabilities $p(x_i | S)$ are indpendent. There are several choices, all of which are implemented in sci-kit learn including Bernoulli, Gaussian, and Multinomial. \n",
    "\n",
    "In particular we will use the Multinomial Distribution: \n",
    " \n",
    "$$ p(\\vec{x}|S_k) = \\frac{(\\sum_i x_i)!}{\\prod_i x_i !}\\prod_i p_{ki}^{xi}$$\n",
    "\n",
    "The Multinomial distribution gives the probability of choosing the histogram defined by $\\vec{x}$ for the class S.\n",
    "\n",
    "Plugging this into the formula above and setting $\\frac{(\\sum_i x_i)!}{\\prod_i x_i !} = a$ we get \n",
    "\n",
    "\\begin{align}\n",
    "p(S | \\vec{x})\n",
    "&=\\frac{a p(S) \\prod_{i=1}^{n}p_{ki}^{x_i}}{\\sum_{k}p(S_k) a \\prod_{i=1}^{n}p_{ki}^{x_i}}\\\\\n",
    "&=\\frac{p(S_k) \\prod_{i=1}^{n}p_{ki}^{x_i}}{\\sum_{k}p(S_k) \\prod_{i=1}^{n}p_{ki}^{x_i}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "which means the prefactor $a$ cancels out! This ends up saving a lot of computational cost in the implementation.\n",
    "\n",
    "By taking the log of the numerator above we arrive at an especially convenient form:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{log}p(S | \\vec{x})\n",
    "&\\propto \\text{log} p(S) + \\sum_{i=1}^{n} {x_i} \\text{log}(p_{ki})\\\\\n",
    "&\\propto b + \\vec{x_i}^{T}\\cdot\\vec{w}\n",
    "\\end{align}\n",
    " \n",
    " \n",
    "which we'll call the joint log likelihood.\n",
    "\n",
    "Hence to implement our model, we simply need to calculate the weight vectors $\\vec{w}$ and the priors $p(S)$ for each class!\n",
    "\n",
    "\n",
    "Now how do we practically calculate $p_{ki}$? This quantity just represents the probability that word $i$ occurs in class $S_k$, which can be estimated by the word counts from the training set:\n",
    "\n",
    "$$ p_{ki} = \\frac{ \\sum_{t}x(t)_{ki} + \\alpha}{\\sum_i \\sum_{t}{x(t)_{ki}} + N_{v} \\alpha} $$\n",
    "\n",
    "where the sum over $t$ represents a sum over all of the training examples in that class $k$. In words, this expression is the number of instances of word $i$ within the class $k$ divided by the total number of all word counts in class $k$. \n",
    "\n",
    "If $p_{ki}$ is zero, we will run into problems since any time $p_{ki}=0$ it will destroy all other information when being multiplied by other $p_{ki}$. Thus, we insert $\\alpha$ which is known as Laplace smoothing. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Training Stage: \n",
    "\n",
    "Inputs: Training Set consisting of $n$ messages which have been converted to $1\\times m$ vectors $\\vec{x}$ along with labels of length $n$, $\\vec{y}$. The Data matrix, $X$ is then composed as $n$ row vecotors of length $m$ giving a $n\\times m$ matrix. \n",
    "\n",
    "Result: Parameterized model: i.e. all of the $p_{ki}$ and $P(S_k)$ for each word in each class.\n",
    "\n",
    "Prediction Stage:\n",
    "\n",
    "Input: Test data arranged as $n\\times m$ matrix X.\n",
    "\n",
    "Output: Probabilities that each input message belongs to each class, in a $n \\times \\#\\text{classes}$ matrix $Y$. \n",
    "\n",
    "\n",
    "\n",
    "### Attributes of class test_MultinomialNB()\n",
    "\n",
    "##### self.W\n",
    "\n",
    "These are the 'weights' for all classes, $k$:\n",
    "\n",
    "\\begin{align}\n",
    "W &= \\begin{bmatrix} \\text{Log}(\\vec{p_{0}}) \\\\\n",
    "                     \\text{Log}(\\vec{p_{1}})\n",
    "                     \\end{bmatrix}\\\\\n",
    " &=\\begin{bmatrix} \\text{Log}(\\frac{ \\sum_{t}x(t)_{ki} + \\alpha}{\\sum_i \\sum_{t}{x(t)_{ki}} + N_{v} \\alpha}) \\\\\n",
    "                     \\text{Log}(\\frac{ \\sum_{t}x(t)_{ki} + \\alpha}{\\sum_i \\sum_{t}{x(t)_{ki}} + N_{v} \\alpha})\n",
    "                     \\end{bmatrix}\\\\\n",
    "&=\\text{log}\\begin{bmatrix} (\\frac{Y^T X^T + \\alpha }{\\sum_i Y^T X^T + \\alpha})\n",
    "                     \\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $p_0$ represents the feature counts for HAM, and $p_1$ is SPAM, for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class test_MultinomialNB():\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha  = alpha\n",
    "    \"\"\"    \n",
    "     We wish to predict the probability that a document\n",
    "     described by feature vector x [1,n_features] belongs\n",
    "     to class C_k. i.e. P(C_k | x)\n",
    "     \n",
    "     Using Bayes Theroem, we can compute the posterior (above) \n",
    "     from the likelihood*prior i.e.\n",
    "     \n",
    "     P( C_k | x ) = P( x | C_k ) * P( C_k ) / P( x )\n",
    "     \n",
    "     or taking the Log:\n",
    "     \n",
    "     Log[ P( C_k | x )] = Log[ P( x | C_k ) ] + Log[ P( C_k ) ] - Log[ P( x ) ]\n",
    "                     = Log[ P( x , C_k ) ] - Log[ Z ]\n",
    "                     = Log[ JL ] - Log[ Z ]\n",
    "                     \n",
    "    where JL means Joint Likelihood.\n",
    "    \n",
    "    JL = P(x, C_k) = P(x1 | x2, x3, ... , C_k)*P(x2 | x3, x4, ... , C_k) * ... * P(xn | C_k) * P(C_k)\n",
    "    \n",
    "    Under the assumption of independent features the conditional probabilities simplify to:\n",
    "    \n",
    "    JL = P(C_k) * prod_i P(x_i | C_k)\n",
    "    \n",
    "    if we assume a multinomial distribution for P(x | C_k) then we have \n",
    "    \n",
    "    JL ~ P(C_k) * prod_i (p_ki ^ x_i)\n",
    "    \n",
    "    or taking Log\n",
    "    \n",
    "    JLL ~ Log[ P(C_k) ] + sum_i (x_i * Log[ p_ki ])\n",
    "    \n",
    "    which in vector form is \n",
    "    \n",
    "    JLL ~ b + dot(x,w_k)\n",
    "    \n",
    "        where w = Log[ p_k ]\n",
    "        \n",
    "    \n",
    "    Z is the 'evidence' which can be computed\n",
    "        \n",
    "    Z = sum_k P( C_k ) * P(x | C_k)\n",
    "      = sum_k JL\n",
    "    which is just the sum of the Joint Likelihood across all classes.\n",
    "    \n",
    "    *** \n",
    "    To fit the model then, we need to compute the matrix 'W' of shape [n_classes, n_features]\n",
    "    where \n",
    "        W_ki = Log [ p_ki ]\n",
    "             = Log [ (sum_{documents in class k} x_i + alpha ) / (sum_{all documents} + n*alpha ]\n",
    "    \n",
    "    We also need the vector 'b'[n_classes,1] which will be a vector of the priors Log[ C_k]\n",
    "    where \n",
    "        b_k = Log[ num_documents in class k / total_num_documents ]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        # Tranform y [n_samples,1] where y_d = label\n",
    "        # to Y [n_samples,n_classes] where  y_dk = 1 if doc d belongs to class k\n",
    "        # else y_dk = 0.\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        Y = Y.astype(np.float64) # cast as floats\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        # Now with Y, feature counts for each class can be computed using \n",
    "        # matrix math i.e.\n",
    "        # fc = Y.T * X\n",
    "        # [n_classes, n_features] = [n_classes, n_samples] * [n_samples, n_features]\n",
    "\n",
    "        feature_counts = safe_sparse_dot(Y.T, X)\n",
    "        \n",
    "        \n",
    "        # Now smooth the feature counts since any zero instance will kill all other non-zero proba's\n",
    "        smooth_fc = feature_counts + self.alpha\n",
    "\n",
    "        # Compute matrix W as Log [ smooth_fc ] - Log [ smooth_fc.sum(axis=1)]\n",
    "\n",
    "\n",
    "        self.W = np.log(smooth_fc) - np.log(smooth_fc.sum(axis=1).reshape(-1,1)) \n",
    "        # Compute vector 'b' as b_k = Log[ n_samples in class k ] - Log[ n_samples ]\n",
    "        self.b = np.log(Y.sum(axis=0)) - np.log(Y.shape[0]) \n",
    "\n",
    "        \n",
    "        \n",
    "    def _JLL(self,X):\n",
    "        # Compute Joint Log Likelihood = b*np.ones((b.shape[0],X)) + W * X.T \n",
    "        # shape _JLL = [ n_classes, n_samples ]\n",
    "        return self.b.reshape(self.b.shape[0],-1) + safe_sparse_dot(self.W,X.T)\n",
    "    def predict(self,X):\n",
    "        return self.classes_[np.argmax(self._JLL(X),axis=0)]\n",
    "    def predict_log_proba(self,X):\n",
    "        log_Z = logsumexp(self._JLL(X),axis=1)\n",
    "        return self._JLL(X) - log_Z #shape?\n",
    "    def predict_proba(self,X):\n",
    "        return np.exp(self.predict_log_proba(X))\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-4c26415873f4>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4c26415873f4>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print classification_report(labels,predictions)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,\\\n",
    "    f1_score, accuracy_score, confusion_matrix\n",
    "data = np.random.randint(10,size=(20,10))\n",
    "labels = np.random.randint(2,size=20)\n",
    "mnb_classifier = test_MultinomialNB()\n",
    "mnb_classifier.fit(data,labels)\n",
    "predictions = mnb_classifier.predict(data)\n",
    "predictions - labels\n",
    "\n",
    "print classification_report(labels,predictions)\n",
    "print confusion_matrix(labels,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "data = np.random.randint(10,size=(20,50))\n",
    "labels = np.random.randint(2,size=20)\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(data,labels)\n",
    "mnb_classifier.predict(data) - labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the equation for proba is $x = \\sum_i x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:50% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:50% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in raw_data['feature_names']:\n",
    "    facet = sns.FacetGrid(data=data,hue='class')\n",
    "    facet.map(sns.kdeplot,feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=data,x=data.target,y=data.alcohol)\n",
    "predictions = wine_detector.predict(data_test)\n",
    "print('accuracy', accuracy_score(label_test, predictions))\n",
    "print('confusion matrix\\n', \n",
    "      confusion_matrix(label_test, predictions))\n",
    "print('(row=expected, col=predicted)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
